#,Question,Answer 
2,But why do the singular values reflect dimensionality (or the lack thereof) ?,"If has rank r, then there will be r non-zero singular values. The reason why this is true is because of how the SVD is structured. The first r singular values correspond to the first r columns of U, which form a basis for the columnspace of A."
3,Can PCA be considered as regression into a fixed dimension space?,"PCA is about finding the best lower dimension space that best approximates the data, this is very similar to regression but we will see more soon about how the detatils work out"
4,"Is this sort of the idea behind that signal processing problem in the previous homework? Like how we projected it onto a lower dimmension to see how the signals grouped to 1 Hz, 3 Hz, 5 Hz?","For that problem, we had these ideal vectors of what the signal should look like (the true sinusoids). In PCA, we do not have access to something like this. However, the general ideas are very similar."
5,I didn't fully catch the movie situation. Can I get a quick summary?,"Yes: Each person has 4 preferences for categories relating to movies. We call this a sensitivity to a movie category. So, we will talk to many people, say n of them. For each person, we determine their rating for particular movies, say there are m of then. We can put these measurments into an m X n matrix. Now, we are going to assume that the person's rating for a movie depends on their sensitivities to different genres. If we assume this structure, particularly that a rating can be seens as the inner product of someone's sensitivities with a vector for the genres of that given movie, then PCA will help us figure out these sensitivities"
6,is the action scores vector all for 1 movie?,"If I am not mistaken, the action score vectors are the a, b, c, and d vectors. These have a length of 100, coressponding to how prominent that category is for each movie."
7,"PCA can be used in situations where the ambient dimensionality is obviously higher than the underlying structure of the data, so PCA prevents overfitting in general? And if that’s the case, even if the underlying structure isn’t obviously a lower dimension, are there ML benefits to using a rank-k approximation in modeling? (aside from memory constraints like the project)","Yes typically, if you can represent your data with a lower dimesnional subspace to a high degree of accuracy (which will coresspond to a sharp cutoff in our singular values), then it will be better to just use the ""simplified data"". This helps us avoid overfiting. But this is not a general rule, as this really does depend on how well we can represent high dimensional data with low dimensional data. In ML, there is also a technique called 'lifting the data', or taking lower dimesnional data and representing it in a higher dimesnional structure. So it really depends but yes, usually a good low dim approximation can help overfitting."
8,what if the largest sigma appears multiple times and has more than one associated vector?,live answered
9,How do we know whether or not we want to look at the columns vs the rows? Like why did we look at the columns in this example?,live answered

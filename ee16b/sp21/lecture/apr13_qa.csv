#,Question,Answer ,Answer 0,Answer 1
2,why don't we do least squares to project onto a lower dimen structure?,"If you go through the derivation from last lecture, you will find that it is similar to least squares, but using the SVD to make the derivation easier."
3,why did we use projection of q1 onto u1 and we didn’t just use u1 as a representation?,"u1 is the principal component, so we are trying to look at the error between q1 and <q1, u1> u1 which is the projection of q1 on u1."
4,"how do we do step 4 (project data onto u1, u2..) is it least squares?","Once u1 and u2 are fixed from the SVD, the projection is given directly by the formula <q1, u1> u1 + <q2, u2> u2",what if we have more q vectors than u vectors?,"sorry I made a typo, That should be q1 instead of q2 in my previous reply.
You apply the same projection formula for every q_i
<q_i, u1> u1 + <q_i, u2> u2"
5,Is this notebook available anywhere?,it's in HW12
6,So these are scalar dot product values being plotted. What would we expect if actually projecting them onto the basis?,you will see a low rank (principal component) approximation of the original data (neuron spikes in this case),"okay, makes sense. So there would be a set of vectors that look somewhat similar representing the first neuron spike. And then there would be a second set of similar vectors representing the other spike right?"
7,what’s the difference between using principal components for columns and for rows?,it depends on the application whether you are interested in analysing the principal components for the columns or the rows
8,Is there a way to measure how good our PCA choice was (of the number of principal components we choose)? Would it essentially be taking the sum of what we were minimizing earlier?,"yes. The number of principal components you choose depends on desired level of accuracy, and your available computatuional resources. Another problem in HW12 shows you how different examples requires different number of principal components to be 'good enough'.",thank you!
9,"by projections, are we talking about the norm of the projected vector?",the projected vector is the projection. We proved that choosing principal components minimizes the norm of the error between the real data and the low rank approximation.
10,Just wondering: can PCA be done using  diagonalization (eigendecomposition) in place of singular value decomposition (as long as we are dealing with a square matrix)?,no. Eigendecomposition is fundamentally different from SVD. I believe another HW12 problem shows you that the eigen decomposition and SVD of the same 2x2 matrix are different.,"okay, thank you"
11,I’m a little confused by how the storage goes down because don’t we need to project the vi’s onto all the columns of our original matrix?,"You only store k=200 singular values and u,v vectors (instead of 900). Then you re-create the image approximation via outer product sum(sigma_i,ui*vi^t) for i=1:200","the projection is already done once we did the SVD. Now we are storing only 200 singular values (for example) instead of the original 900, so that's a lot of compression. We can't recover the original data anymore",Got it thank you!
12,"I'm trying to get an intuitive understanding of what SVD does differently from diagonalization, to clarify: and U and V matrices are always rotations (because of the orthonormal columns) and the diagonal sigma matrix is always non-negative (unlike the eigenvalue matrix in diagonalization)

So my understanding is diagonalization is a change an eigenbasis, scaling, and change back. Whereas SVD is a rotation, stretch/squish, and rotation back. Is that an accurate intuitive understanding?","That is a good intuition with some minor inaccuracy. In SVD, the U and V are not a 'rotation' and 'rotation back', it's more like 'rotation' and then 'some more rotation'. Tomorrow's Dis 12B will walk you through this geometric interpretation.","Okay cool, thanks","Also note that since U is m x m and V is n x n, they rotate in different subspaces if m != n"
13,Just to double check w is just some arbitrary vector so we can dot product and find a label dependent on xi?,"w is not completely arbitrary, you can think of it as some principal component that we are projecting all the vectors xi on to"
14,What does w represent again,you can think of w as some principal component that we are projecting all the vectors xi on to
15,what exactly are the w vector and b here?,w is some principal component that we are projecting all the vectors xi on to; b is the projection (inner product) that we are classifying
16,where can we access these demos?,"the classifier demos are in HW13, the PCA demos are in HW12"
17,"when might we start to think we actually have 3 clusters , since that second blue cluster is pretty far","We are just doing binary classification here (2 clusters), i.e. we know there are only 2 possible clusters but need to figure out the best classification boundary between the 2"
18,What exactly is meant by cost of the LSQ?,the minimized value
